üí° Scenario: you‚Äôre designing a payment API (POST /charge) for a startup. clients (mobile apps, web) may retry requests if the network flakes.

üëâ Q:

what are idempotency keys?
An idempotency key is a unique client-generated token like an uuid sent with a request for example: POST/charge
-The purpose is to ensure that retires due to network issues or client crashed dont accidentally create duplicate side effectts like charge the customer twice
-Rule is if the same client sends the same key again, the server should return the same result it gave the frist time.

how do they make retries safe?
Without idempotency: The client retires -> the server might create duplicate payment records

with idempotency: the client sends the same key in the header ex:idempotency-key:123abc
The server checks: "Have i seen the key 123abc before?
--If yes:return stored response
--If no: process the requesr, save the result against that key

---The retires are safe, becuase they dont recexute the action

what‚Äôs the basic flow (server side)?
1.client sends request with key
2.server cehcks stores(eg. Redis or DB table idempotency keys)
3.If key exists with response -> return that response
4.If key doesnt exist -> process normally, save {key, request_hash, response, sttus}
5.future identical key + request => serve cached response
6/ If same key but diffenret boy -> return 409 conflict(client bug)
what are trade-offs or design choices (storage, TTL, collisions, modified body, scale)?

Storage: 
redis(fast, ttl for expiry)
sql/nosql table(durable, but heavier)

TTL(Time To Live):
keys usaully expire after hours-days(eg. 24-72hr)

Scope:
Global or per user(per user is safer)

Modified Body:'Must detecct and reject(prevents key re-use with diffenret data)

Scale:
Hot keys(lots of retires at once) need locking to avoid race conditions

Consistency:
In multi-region systems, you need repplication(so retires in a different region still see the same key)

Example Analogy:
Think of idempotency keys like a ‚Äúreceipt number.‚Äù If you retry a purchase with the same receipt number, the cashier won‚Äôt charge you again ‚Äî they‚Äôll just show you the receipt you already got.

MOCK:
‚ÄúYou‚Äôre designing a payments API (POST /charge). Clients might retry requests if they lose the response. 
How would you make sure retries don‚Äôt double-charge the user?‚Äù

I‚Äôd make the API idempotent by requiring clients to send a unique idempotency key with each POST /charge. On the server, 
I store that key with the request and its response. If a retry comes in with the same key,
I return the stored response instead of charging again. For fast lookup and expiry
I could use Redis with TTL, but for long-lived or very large datasets,
a SQL/NoSQL store is safer, at the cost of higher latency.

Design a rate limiter for an API service that allows, say, 100 requests per user per minute. How would you build it?
Id implement this with a toekn bucket in redis. each user has a bucket refilled at a fixed rate for exmaple: 100 tokens per minute. Every
request consumes one token, and if the bucket is empty the requst is rejected with 429 too many requests.
Redis is ideal here becuase it supports atomic incrementss and ttls for fast expiry. Comapred to fixed windows,
this smooths out burts and is production ready. If scaling glovally, id shard buckets across redis nodes and replicate for HA. This ensures
fairness per user while keeping latency low.


Webhook Retries + Dead Letter Queues (DLQ)
When a system sends webhooks (event notifications, payments, updates), the receiving service might be temporarily down or slow.
If you simply retry blindly:

You might duplicate side-effects (e.g., charge a card twice).

You could overwhelm the downstream service with repeated failures.

Hence we need a retry system that‚Äôs idempotent, safe, and bounded.

In a production webhook system, I‚Äôd use an exponential backoff retry policy with a max limit,
combined with idempotency keys so repeated deliveries don‚Äôt duplicate side-effects. 
Failed deliveries beyond the retry limit would go into a Dead Letter Queue for manual inspection or delayed replay. 
A typical stack might use Redis for short-term retry scheduling and a Kafka DLQ or SQL table for long-term persistence. 
This design guarantees reliability without flooding the downstream service or losing events.


Design a comment system
I‚Äôd design a comment system around a Comment table keyed by post_id and parent_id to support nesting.
For scaling, I‚Äôd paginate by created_at, cache top comments in Redis, and shard by post ID.
Moderation uses a flag table with async review, and deletions are soft to maintain thread integrity.
For reliability, new comments trigger background jobs for notifications and spam checks.
This approach scales to millions of comments per post while staying consistent and fast.

Rate limiting prevents users or systems from sending too many requests to a service in a short time.
The most common approach is the **Token Bucket algorithm**, where each user has a ‚Äúbucket‚Äù 
that refills tokens at a fixed rate, and each request consumes one token. When the bucket is empty, 
further requests are rejected (e.g., returning HTTP 429). This ensures fair usage and protects 
servers from overload. It‚Äôs usually implemented with fast storage like **Redis**, which tracks 
token counts per user with TTL for expiration.

CACHING:
Caching is a technique used to store frequently accessed data in fast, temporary memory (like Redis or Memcached) to reduce the number of direct database queries and 
improve application performance. By keeping popular or recently used data in memory, a cache can serve requests much faster 
than a traditional database, significantly lowering latency and improving scalability.

There are several common caching patterns. The cache-aside (lazy loading) pattern is the most common, where the application checks the cache first and, on a miss, 
retrieves data from the database and updates the cache. The read-through pattern lets the cache itself handle loading data on misses automatically. 
Write-through ensures consistency by writing data to both the cache and the database at the same time, while write-back (lazy write) writes 
only to the cache and syncs with the database later‚Äîthis offers speed but risks data loss on failure.

To keep cached data accurate, systems use TTL (Time-To-Live) to automatically expire stale entries, and eviction policies like LRU (Least Recently Used) or LFU (Least Frequently Used) 
to manage space efficiently. It‚Äôs also important to prevent issues like cache stampedes, 
where many clients request the same data when it expires‚Äîthis can be mitigated using
locks, request coalescing, or adding random TTL ‚Äújitter.‚Äù

In production environments, Redis is widely preferred due to its persistence, advanced data structures, and TTL features,
while Memcached offers a lightweight, purely in-memory alternative for ephemeral caching.


Database Indexing

A database index is a data structure (usually a B-tree or hash table) that improves query performance by allowing faster lookups, sorts, and joins.
It works like a book index ‚Äî instead of scanning the whole table, the database can jump directly to the relevant rows using sorted keys or hash mappings.

How it works:
When an index is created on a column (e.g., `email`), the DB stores a sorted list of values with pointers to the full rows.
Queries that use that column in a WHERE or JOIN clause can find results in **O(log n)** instead of O(n).

Pros: Faster reads, optimized queries, efficient range lookups
Cons: Slower writes (updates must modify indexes), extra storage usage

Common types:

B-tree index** ‚Äì supports range and equality queries (default in SQL)
Hash index** ‚Äì faster for exact matches, not for ranges
Composite index ‚Äì covers multiple columns (order matters)
Full-text index ‚Äì for text search and keyword queries

Ex.
CREATE INDEX idx_user_email ON users (email);

Key takeaway:

--Indexes accelerate read-heavy workloads by reducing table scans but increase write overhead. Use them strategically on frequently queried columns.


Caching systems
Caching stores frequently accessed data in high-speed memory (like Redis or Memcached) to improve application performance and reduce load on slower data sources.
When a user requests data, the system first checks the cache ‚Äî if found (cache hit), the data is returned instantly; if not (cache miss), 
the app fetches it from the database and stores it in the cache for future use. Common strategies include cache-aside (lazy loading), 
write-through (update cache and DB together), and write-back (write to cache first, then sync to DB). Proper cache design uses TTLs (time-to-live) and eviction 
policies such as LRU to avoid stale or overflowed data. While caching speeds up reads and improves scalability, it must be balanced carefully 
to prevent data inconsistency and excessive memory use.

Message queues

Message queues enable asynchronous communication between services using a producer‚Äìconsumer model. A producer sends messages to a queue,
and consumers process them later, allowing systems to scale and stay resilient. Common tools include RabbitMQ, Kafka, and AWS SQS. 
MQs decouple services, handle spikes in load, and prevent data loss through persistent queues. They‚Äôre key in event-driven architectures where 
components react to messages rather than direct calls. However, they add operational complexity and must handle retries, ordering, and potential duplicates carefully.

Load Balancing

Load balancing distributes incoming requests across multiple servers to maximize reliability, performance, and availability. 
Common algorithms include Round Robin (equal rotation), Least Connections (lowest load), and Weighted strategies for capacity-based routing. 
Load balancers can operate at Layer 4 (TCP) or Layer 7 (HTTP) depending on how deeply they inspect traffic. 
They improve scalability and fault tolerance but must handle challenges like sticky sessions and avoiding single points of failure. 
Real-world tools include NGINX, HAProxy, AWS ELB, and Cloudflare.