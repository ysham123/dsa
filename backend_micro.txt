üí° Scenario: you‚Äôre designing a payment API (POST /charge) for a startup. clients (mobile apps, web) may retry requests if the network flakes.

üëâ Q:

what are idempotency keys?
An idempotency key is a unique client-generated token like an uuid sent with a request for example: POST/charge
-The purpose is to ensure that retires due to network issues or client crashed dont accidentally create duplicate side effectts like charge the customer twice
-Rule is if the same client sends the same key again, the server should return the same result it gave the frist time.

how do they make retries safe?
Without idempotency: The client retires -> the server might create duplicate payment records

with idempotency: the client sends the same key in the header ex:idempotency-key:123abc
The server checks: "Have i seen the key 123abc before?
--If yes:return stored response
--If no: process the requesr, save the result against that key

---The retires are safe, becuase they dont recexute the action

what‚Äôs the basic flow (server side)?
1.client sends request with key
2.server cehcks stores(eg. Redis or DB table idempotency keys)
3.If key exists with response -> return that response
4.If key doesnt exist -> process normally, save {key, request_hash, response, sttus}
5.future identical key + request => serve cached response
6/ If same key but diffenret boy -> return 409 conflict(client bug)
what are trade-offs or design choices (storage, TTL, collisions, modified body, scale)?

Storage: 
redis(fast, ttl for expiry)
sql/nosql table(durable, but heavier)

TTL(Time To Live):
keys usaully expire after hours-days(eg. 24-72hr)

Scope:
Global or per user(per user is safer)

Modified Body:'Must detecct and reject(prevents key re-use with diffenret data)

Scale:
Hot keys(lots of retires at once) need locking to avoid race conditions

Consistency:
In multi-region systems, you need repplication(so retires in a different region still see the same key)

Example Analogy:
Think of idempotency keys like a ‚Äúreceipt number.‚Äù If you retry a purchase with the same receipt number, the cashier won‚Äôt charge you again ‚Äî they‚Äôll just show you the receipt you already got.

MOCK:
‚ÄúYou‚Äôre designing a payments API (POST /charge). Clients might retry requests if they lose the response. 
How would you make sure retries don‚Äôt double-charge the user?‚Äù

I‚Äôd make the API idempotent by requiring clients to send a unique idempotency key with each POST /charge. On the server, 
I store that key with the request and its response. If a retry comes in with the same key,
I return the stored response instead of charging again. For fast lookup and expiry
I could use Redis with TTL, but for long-lived or very large datasets,
a SQL/NoSQL store is safer, at the cost of higher latency.

Design a rate limiter for an API service that allows, say, 100 requests per user per minute. How would you build it?
Id implement this with a toekn bucket in redis. each user has a bucket refilled at a fixed rate for exmaple: 100 tokens per minute. Every
request consumes one token, and if the bucket is empty the requst is rejected with 429 too many requests.
Redis is ideal here becuase it supports atomic incrementss and ttls for fast expiry. Comapred to fixed windows,
this smooths out burts and is production ready. If scaling glovally, id shard buckets across redis nodes and replicate for HA. This ensures
fairness per user while keeping latency low.


Webhook Retries + Dead Letter Queues (DLQ)
When a system sends webhooks (event notifications, payments, updates), the receiving service might be temporarily down or slow.
If you simply retry blindly:

You might duplicate side-effects (e.g., charge a card twice).

You could overwhelm the downstream service with repeated failures.

Hence we need a retry system that‚Äôs idempotent, safe, and bounded.

In a production webhook system, I‚Äôd use an exponential backoff retry policy with a max limit,
combined with idempotency keys so repeated deliveries don‚Äôt duplicate side-effects. 
Failed deliveries beyond the retry limit would go into a Dead Letter Queue for manual inspection or delayed replay. 
A typical stack might use Redis for short-term retry scheduling and a Kafka DLQ or SQL table for long-term persistence. 
This design guarantees reliability without flooding the downstream service or losing events.


Design a comment system
I‚Äôd design a comment system around a Comment table keyed by post_id and parent_id to support nesting.
For scaling, I‚Äôd paginate by created_at, cache top comments in Redis, and shard by post ID.
Moderation uses a flag table with async review, and deletions are soft to maintain thread integrity.
For reliability, new comments trigger background jobs for notifications and spam checks.
This approach scales to millions of comments per post while staying consistent and fast.